{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":492603,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":391538,"modelId":410251}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install required packages","metadata":{}},{"cell_type":"code","source":"!pip install gradio ultralytics paddlepaddle paddleocr transformers torch torchvision accelerate bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:42:52.106880Z","iopub.execute_input":"2025-07-28T12:42:52.107558Z","iopub.status.idle":"2025-07-28T12:44:33.080051Z","shell.execute_reply.started":"2025-07-28T12:42:52.107537Z","shell.execute_reply":"2025-07-28T12:44:33.079330Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\nCollecting ultralytics\n  Downloading ultralytics-8.3.170-py3-none-any.whl.metadata (37 kB)\nCollecting paddlepaddle\n  Downloading paddlepaddle-3.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (8.8 kB)\nCollecting paddleocr\n  Downloading paddleocr-3.1.0-py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\nRequirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.13)\nRequirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\nRequirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\nRequirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\nRequirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\nRequirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\nRequirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\nRequirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.0)\nRequirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\nRequirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\nRequirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\nRequirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.5.1)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.4)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (3.20.3)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (4.4.2)\nCollecting opt_einsum==3.3.0 (from paddlepaddle)\n  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (3.5)\nCollecting paddlex>=3.1.0 (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading paddlex-3.1.3-py3-none-any.whl.metadata (78 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (5.2.0)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (6.9.0)\nRequirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.16.0)\nCollecting ruamel.yaml (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (5.10.0)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.8.1)\nCollecting ftfy (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting GPUtil>=1.4 (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting opencv-contrib-python==4.10.0.84 (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nCollecting pypdfium2>=4 (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.9.0)\nRequirement already satisfied: imagesize in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.4.1)\nRequirement already satisfied: langchain>=0.2 in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.3.26)\nCollecting langchain-community>=0.2 (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.3.66)\nCollecting langchain-openai>=0.1 (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (5.4.0)\nRequirement already satisfied: openai>=1.63 in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.91.0)\nRequirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.1.5)\nCollecting premailer (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading premailer-3.10.0-py2.py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: pyclipper in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.3.0.post6)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.2.2)\nRequirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.1.1)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (4.13.4)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.3.8)\nRequirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.4.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.0.41)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.12.13)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (8.5.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.33)\nCollecting packaging (from gradio)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting langchain-core (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading langchain_core-0.3.72-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.10.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.7)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.2.13)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.0.0)\nCollecting cssselect (from premailer->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\nCollecting cssutils (from premailer->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading cssutils-2.11.1-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from premailer->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (5.5.2)\nCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.6.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.20.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.9.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.0.0)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.23.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.2.3)\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from cssutils->premailer->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (10.7.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.1.0)\nDownloading ultralytics-8.3.170-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading paddlepaddle-3.1.0-cp311-cp311-manylinux1_x86_64.whl (195.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.0/195.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading paddleocr-3.1.0-py3-none-any.whl (70 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading paddlex-3.1.3-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\nDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.72-py3-none-any.whl (442 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading premailer-3.10.0-py2.py3-none-any.whl (19 kB)\nDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\nDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\nDownloading cssutils-2.11.1-py3-none-any.whl (385 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.7/385.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=6c48cc08f1f571bc255d265ddf35e9c2a71d6a1bf9e451635a84ba4cd092e1e3\n  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil, ruamel.yaml.clib, python-dotenv, pypdfium2, packaging, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, httpx-sse, ftfy, cssutils, cssselect, ruamel.yaml, premailer, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pydantic-settings, nvidia-cusolver-cu12, langchain-core, langchain-openai, paddlex, opencv-contrib-python, langchain-community, ultralytics-thop, opt_einsum, ultralytics, paddlepaddle, paddleocr, bitsandbytes\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.66\n    Uninstalling langchain-core-0.3.66:\n      Successfully uninstalled langchain-core-0.3.66\n  Attempting uninstall: opencv-contrib-python\n    Found existing installation: opencv-contrib-python 4.11.0.86\n    Uninstalling opencv-contrib-python-4.11.0.86:\n      Successfully uninstalled opencv-contrib-python-4.11.0.86\n  Attempting uninstall: opt_einsum\n    Found existing installation: opt_einsum 3.4.0\n    Uninstalling opt_einsum-3.4.0:\n      Successfully uninstalled opt_einsum-3.4.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed GPUtil-1.4.0 bitsandbytes-0.46.1 cssselect-1.3.0 cssutils-2.11.1 ftfy-6.3.1 httpx-sse-0.4.1 langchain-community-0.3.27 langchain-core-0.3.72 langchain-openai-0.3.28 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opencv-contrib-python-4.10.0.84 opt_einsum-3.3.0 packaging-24.2 paddleocr-3.1.0 paddlepaddle-3.1.0 paddlex-3.1.3 premailer-3.10.0 pydantic-settings-2.10.1 pypdfium2-4.30.0 python-dotenv-1.1.1 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 ultralytics-8.3.170 ultralytics-thop-2.0.14\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport cv2\nimport json\nfrom PIL import Image\nimport gradio as gr\nfrom ultralytics import YOLO\nfrom paddleocr import PaddleOCR\nfrom transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig\nimport warnings\nimport re\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:44:33.081547Z","iopub.execute_input":"2025-07-28T12:44:33.081803Z","iopub.status.idle":"2025-07-28T12:45:05.384231Z","shell.execute_reply.started":"2025-07-28T12:44:33.081779Z","shell.execute_reply":"2025-07-28T12:45:05.383626Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753706694.942312      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753706694.995738      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/license_plate_detect_yolo11/pytorch/default/1/best.pt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Load Models","metadata":{}},{"cell_type":"code","source":"# Global variables to store loaded models\nMODELS_LOADED = False\nyolo_model = None\nllava_model = None\nprocessor = None\nocr_model = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:45:05.384874Z","iopub.execute_input":"2025-07-28T12:45:05.385509Z","iopub.status.idle":"2025-07-28T12:45:05.389256Z","shell.execute_reply.started":"2025-07-28T12:45:05.385488Z","shell.execute_reply":"2025-07-28T12:45:05.388540Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def load_models():\n    \"\"\"Load all required models once\"\"\"\n    global MODELS_LOADED, yolo_model, llava_model, processor, ocr_model\n    \n    if MODELS_LOADED:\n        return yolo_model, llava_model, processor, ocr_model\n    \n    print(\"Loading models...\")\n    \n    try:\n        # Load YOLO model for license plate detection\n        yolo_model_path = \"/kaggle/input/license_plate_detect_yolo11/pytorch/default/1/best.pt\"\n        yolo_model = YOLO(yolo_model_path)\n        print(\"✅ YOLO model loaded successfully\")\n\n        # Load LLaVA-NeXT model with quantization for faster inference\n        quant_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_quant_type=\"nf4\"\n        )\n        \n        processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n        llava_model = AutoModelForImageTextToText.from_pretrained(\n            \"llava-hf/llava-v1.6-mistral-7b-hf\", \n            quantization_config=quant_config, \n            device_map=\"auto\",\n            torch_dtype=torch.float16\n        )\n        print(\"✅ LLaVA-NeXT model loaded successfully\")\n        \n        # Initialize PaddleOCR\n        ocr_model = PaddleOCR(\n            use_angle_cls=True, \n            lang='en'\n        )\n        print(\"✅ PaddleOCR model loaded successfully\")\n        \n        MODELS_LOADED = True\n        return yolo_model, llava_model, processor, ocr_model\n        \n    except Exception as e:\n        print(f\"Error loading models: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:45:05.390675Z","iopub.execute_input":"2025-07-28T12:45:05.390910Z","iopub.status.idle":"2025-07-28T12:45:05.417900Z","shell.execute_reply.started":"2025-07-28T12:45:05.390889Z","shell.execute_reply":"2025-07-28T12:45:05.417215Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Detect License Plates","metadata":{}},{"cell_type":"code","source":"def detect_license_plates(image, yolo_model, confidence_threshold=0.5):\n    \"\"\"Detect license plates using YOLO model with adjustable confidence threshold\"\"\"\n    try:\n        # Convert PIL image to numpy array\n        if isinstance(image, Image.Image):\n            image_np = np.array(image)\n        else:\n            image_np = image\n        \n        print(f\"Running YOLO detection with confidence threshold: {confidence_threshold}\")\n        \n        # Run YOLO inference with user-specified confidence threshold\n        results = yolo_model(\n            image_np, \n            conf=confidence_threshold, \n            imgsz=640,\n            half=False,  # Disable FP16 for better compatibility\n            device='cuda' if torch.cuda.is_available() else 'cpu',\n            verbose=True\n        )\n        \n        detections = []\n        for result in results:\n            boxes = result.boxes\n            if boxes is not None:\n                print(f\"Found {len(boxes)} potential detections\")\n                for i, box in enumerate(boxes):\n                    # Get bounding box coordinates\n                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                    confidence = box.conf[0].cpu().numpy()\n                    \n                    # More lenient size filter\n                    width = x2 - x1\n                    height = y2 - y1\n                    print(f\"Detection {i+1}: bbox=({x1:.1f},{y1:.1f},{x2:.1f},{y2:.1f}), size=({width:.1f}x{height:.1f}), conf={confidence:.3f}\")\n                    \n                    if width > 15 and height > 8:  # More lenient minimum size filter\n                        detections.append({\n                            'bbox': [int(x1), int(y1), int(x2), int(y2)],\n                            'confidence': float(confidence)\n                        })\n                        print(f\"✅ Added detection {i+1}\")\n                    else:\n                        print(f\"❌ Filtered out detection {i+1} (too small)\")\n            else:\n                print(\"No boxes detected by YOLO\")\n        \n        print(f\"Final detections: {len(detections)}\")\n        return detections\n    except Exception as e:\n        print(f\"Error in license plate detection: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:45:05.418613Z","iopub.execute_input":"2025-07-28T12:45:05.418827Z","iopub.status.idle":"2025-07-28T12:45:05.429463Z","shell.execute_reply.started":"2025-07-28T12:45:05.418812Z","shell.execute_reply":"2025-07-28T12:45:05.428874Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Preprocess License Plates","metadata":{}},{"cell_type":"code","source":"def preprocess_plate_image(plate_image):\n    \"\"\"Preprocess license plate image for better OCR\"\"\"\n    try:\n        # Ensure we have a valid image\n        if plate_image is None or plate_image.size == 0:\n            return None\n            \n        # Convert to grayscale if needed\n        if len(plate_image.shape) == 3:\n            gray = cv2.cvtColor(plate_image, cv2.COLOR_RGB2GRAY)\n        else:\n            gray = plate_image\n        \n        # Resize if too small\n        h, w = gray.shape\n        if h < 32 or w < 64:\n            scale_factor = max(32/h, 64/w, 2.0)\n            new_h, new_w = int(h * scale_factor), int(w * scale_factor)\n            gray = cv2.resize(gray, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n        \n        # Apply CLAHE for better contrast\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n        enhanced = clahe.apply(gray)\n        \n        # Apply Gaussian blur to reduce noise\n        blurred = cv2.GaussianBlur(enhanced, (3, 3), 0)\n        \n        # Apply threshold\n        _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n        \n        # Convert back to 3-channel for PaddleOCR compatibility\n        thresh_3ch = cv2.cvtColor(thresh, cv2.COLOR_GRAY2RGB)\n        return thresh_3ch\n        \n    except Exception as e:\n        print(f\"Error preprocessing plate image: {str(e)}\")\n        return plate_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:45:05.430185Z","iopub.execute_input":"2025-07-28T12:45:05.430819Z","iopub.status.idle":"2025-07-28T12:45:05.443074Z","shell.execute_reply.started":"2025-07-28T12:45:05.430792Z","shell.execute_reply":"2025-07-28T12:45:05.442399Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Extract Plate Text","metadata":{}},{"cell_type":"code","source":"def extract_text_from_plate(image, bbox, ocr_model, ocr_confidence_threshold=0.1):\n    \"\"\"Extract text from license plate using OCR with adjustable confidence threshold\"\"\"\n    try:\n        x1, y1, x2, y2 = bbox\n        \n        # Convert PIL to numpy if needed\n        if isinstance(image, Image.Image):\n            image_np = np.array(image)\n        else:\n            image_np = image\n        \n        # Get original image dimensions\n        h, w = image_np.shape[:2]\n        print(f\"Original image size: {w}x{h}\")\n        print(f\"Raw bbox: ({x1},{y1},{x2},{y2})\")\n        \n        # Clamp coordinates to image boundaries\n        x1_clamped = max(0, min(x1, w-1))\n        y1_clamped = max(0, min(y1, h-1))\n        x2_clamped = max(x1_clamped+1, min(x2, w))\n        y2_clamped = max(y1_clamped+1, min(y2, h))\n        \n        print(f\"Clamped bbox: ({x1_clamped},{y1_clamped},{x2_clamped},{y2_clamped})\")\n        \n        # Check if bbox is still valid after clamping\n        if x2_clamped <= x1_clamped or y2_clamped <= y1_clamped:\n            print(\"Invalid bbox coordinates after clamping\")\n            return \"\", 0.0\n        \n        # Crop the license plate region with some padding\n        padding = 10\n        x1_pad = max(0, x1_clamped - padding)\n        y1_pad = max(0, y1_clamped - padding)\n        x2_pad = min(w, x2_clamped + padding)\n        y2_pad = min(h, y2_clamped + padding)\n        \n        print(f\"Final crop region: ({x1_pad},{y1_pad},{x2_pad},{y2_pad})\")\n        \n        plate_image = image_np[y1_pad:y2_pad, x1_pad:x2_pad]\n        \n        # Check if plate_image is valid\n        if plate_image.size == 0:\n            print(\"Empty plate image after cropping\")\n            return \"\", 0.0\n        \n        print(f\"Cropped plate image size: {plate_image.shape}\")\n        \n        # Ensure the cropped image has the right format\n        if len(plate_image.shape) != 3:\n            print(\"Converting grayscale to RGB\")\n            if len(plate_image.shape) == 2:\n                plate_image = cv2.cvtColor(plate_image, cv2.COLOR_GRAY2RGB)\n        \n        # Try OCR on original cropped image first\n        best_text = \"\"\n        best_confidence = 0.0\n        \n        try:\n            print(f\"Trying OCR on original image with confidence threshold: {ocr_confidence_threshold}\")\n            ocr_results_original = ocr_model.ocr(plate_image)\n\n            # Handle the new PaddleOCR dict format\n            if isinstance(ocr_results_original, dict):\n                rec_texts = ocr_results_original.get('rec_texts', [])\n                rec_scores = ocr_results_original.get('rec_scores', [])\n                \n                print(f\"Found rec_texts: {rec_texts}\")\n                print(f\"Found rec_scores: {rec_scores}\")\n                \n                for i, text in enumerate(rec_texts):\n                    if i < len(rec_scores):\n                        conf = rec_scores[i]\n                        if text and len(str(text).strip()) > 0 and conf > ocr_confidence_threshold:\n                            print(f\"  Original - Text: '{text}', Confidence: {conf:.3f}\")\n                            \n                            # Clean up the text\n                            cleaned_text = re.sub(r'[^A-Z0-9]', '', str(text).upper())\n                            \n                            if cleaned_text and len(cleaned_text) >= 2:\n                                if conf > best_confidence:\n                                    best_text = cleaned_text\n                                    best_confidence = conf\n                                    print(f\"  ✅ New best result: '{best_text}' (conf: {best_confidence:.3f})\")\n            \n            # Handle list format (legacy or different version)\n            elif isinstance(ocr_results_original, list) and len(ocr_results_original) > 0:\n                print(\"Handling list format...\")\n                \n                # Check if it's a list of detection results\n                if isinstance(ocr_results_original[0], list):\n                    for line in ocr_results_original[0]:\n                        if line and len(line) >= 2 and line[1]:\n                            text = str(line[1][0]).strip()\n                            conf = float(line[1][1]) if len(line[1]) >= 2 else 0.0\n                            \n                            print(f\"  Original (list) - Text: '{text}', Confidence: {conf:.3f}\")\n                            \n                            if conf > ocr_confidence_threshold and len(text) > 0:\n                                cleaned_text = re.sub(r'[^A-Z0-9]', '', text.upper())\n                                \n                                if cleaned_text and len(cleaned_text) >= 2:\n                                    if conf > best_confidence:\n                                        best_text = cleaned_text\n                                        best_confidence = conf\n                                        print(f\"  ✅ New best result: '{best_text}' (conf: {best_confidence:.3f})\")\n                \n                # Direct list of results\n                else:\n                    for item in ocr_results_original:\n                        if isinstance(item, dict):\n                            rec_texts = item.get('rec_texts', [])\n                            rec_scores = item.get('rec_scores', [])\n                            \n                            for i, text in enumerate(rec_texts):\n                                if i < len(rec_scores):\n                                    conf = rec_scores[i]\n                                    if text and len(str(text).strip()) > 0 and conf > ocr_confidence_threshold:\n                                        cleaned_text = re.sub(r'[^A-Z0-9]', '', str(text).upper())\n                                        if cleaned_text and len(cleaned_text) >= 2:\n                                            if conf > best_confidence:\n                                                best_text = cleaned_text\n                                                best_confidence = conf\n                                                print(f\"  ✅ New best result: '{best_text}' (conf: {best_confidence:.3f})\")\n            \n        except Exception as e:\n            print(f\"Error with original OCR: {str(e)}\")\n\n        \n        # Try OCR on preprocessed image\n        if best_confidence < 0.8: \n            try:\n                print(\"Trying OCR on preprocessed image...\")\n                processed_plate = preprocess_plate_image(plate_image)\n                \n                if processed_plate is not None:\n                    ocr_results_processed = ocr_model.ocr(processed_plate)\n                    \n                    print(f\"Processed OCR result type: {type(ocr_results_processed)}\")\n                    \n                    # Handle the new PaddleOCR dict format\n                    if isinstance(ocr_results_processed, dict):\n                        rec_texts = ocr_results_processed.get('rec_texts', [])\n                        rec_scores = ocr_results_processed.get('rec_scores', [])\n                        \n                        print(f\"Processed rec_texts: {rec_texts}\")\n                        print(f\"Processed rec_scores: {rec_scores}\")\n                        \n                        for i, text in enumerate(rec_texts):\n                            if i < len(rec_scores):\n                                conf = rec_scores[i]\n                                if text and len(str(text).strip()) > 0 and conf > ocr_confidence_threshold:\n                                    print(f\"  Processed - Text: '{text}', Confidence: {conf:.3f}\")\n                                    \n                                    # Clean up the text\n                                    cleaned_text = re.sub(r'[^A-Z0-9]', '', str(text).upper())\n                                    \n                                    if cleaned_text and len(cleaned_text) >= 2:\n                                        if conf > best_confidence:\n                                            best_text = cleaned_text\n                                            best_confidence = conf\n                                            print(f\"  ✅ New best result: '{best_text}' (conf: {best_confidence:.3f})\")\n                    \n                    # Handle other formats...\n                    elif isinstance(ocr_results_processed, list) and len(ocr_results_processed) > 0:\n                        # Similar logic as above for processed results\n                        if isinstance(ocr_results_processed[0], list):\n                            for line in ocr_results_processed[0]:\n                                if line and len(line) >= 2 and line[1]:\n                                    text = str(line[1][0]).strip()\n                                    conf = float(line[1][1]) if len(line[1]) >= 2 else 0.0\n                                    \n                                    if conf > ocr_confidence_threshold and len(text) > 0:\n                                        cleaned_text = re.sub(r'[^A-Z0-9]', '', text.upper())\n                                        if cleaned_text and len(cleaned_text) >= 2:\n                                            if conf > best_confidence:\n                                                best_text = cleaned_text\n                                                best_confidence = conf\n                                                print(f\"  ✅ New best result: '{best_text}' (conf: {best_confidence:.3f})\")\n                        \n            except Exception as e:\n                print(f\"Error with preprocessed OCR: {str(e)}\")\n\n        print(f\"Final result: '{best_text}' (conf: {best_confidence:.3f})\")\n        return best_text, best_confidence\n    \n    except Exception as e:\n        print(f\"Error in OCR text extraction: {str(e)}\")\n        return \"\", 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:45:05.444083Z","iopub.execute_input":"2025-07-28T12:45:05.444337Z","iopub.status.idle":"2025-07-28T12:45:05.464937Z","shell.execute_reply.started":"2025-07-28T12:45:05.444315Z","shell.execute_reply":"2025-07-28T12:45:05.464196Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Traffic Scene Description","metadata":{}},{"cell_type":"code","source":"def describe_traffic_scene(image, llava_model, processor, temperature=0.7, top_p=0.9):\n    \"\"\"Generate traffic scene description using LLaVA-NeXT with adjustable temperature and top_p\"\"\"\n    try:\n        # Resize image if too large to speed up inference\n        if isinstance(image, Image.Image):\n            width, height = image.size\n            if width > 1024 or height > 1024:\n                image.thumbnail((1024, 1024), Image.Resampling.LANCZOS)\n        \n        # Simplified prompt for faster processing\n        prompt = \"\"\"\n        USER: <image>\n        Analyze this traffic scene in detail. Describe:\n        1. Types of vehicles present (cars, trucks, motorcycles, etc.)\n        2. Traffic signs, signals, and road markings visible\n        3. Road conditions and infrastructure\n        4. Weather and lighting conditions\n        5. Overall traffic flow and density\n        6. Any notable safety considerations or hazards\n        \n        ASSISTANT:\"\"\"\n        \n        inputs = processor(image, prompt, return_tensors=\"pt\")\n        \n        # Move to appropriate device\n        device = next(llava_model.parameters()).device\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        print(f\"Generating scene description with temperature={temperature}, top_p={top_p}\")\n        \n        with torch.inference_mode():\n            # Use adjustable temperature and top_p parameters\n            output = llava_model.generate(\n                **inputs, \n                max_new_tokens=150, \n                do_sample=True if temperature > 0.0 else False,\n                temperature=temperature if temperature > 0.0 else None,\n                top_p=top_p if temperature > 0.0 else None,\n                pad_token_id=processor.tokenizer.eos_token_id\n            )\n        \n        full_response = processor.decode(output[0], skip_special_tokens=True)\n        \n        # Extract only the assistant's response\n        description = \"\"\n        if \"ASSISTANT:\" in full_response:\n            parts = full_response.split(\"ASSISTANT:\")\n            if len(parts) > 1:\n                description = parts[-1].strip()\n        \n        # Clean up any remaining template artifacts\n        if description.startswith(\"[INST]\") or description.startswith(\"USER:\"):\n            # Try alternative extraction\n            lines = full_response.split('\\n')\n            for i, line in enumerate(lines):\n                if \"ASSISTANT:\" in line and i + 1 < len(lines):\n                    description = '\\n'.join(lines[i+1:]).strip()\n                    break\n        \n        # Remove any remaining template markers\n        description = re.sub(r'\\[/?INST\\]', '', description).strip()\n        description = re.sub(r'USER:.*?ASSISTANT:', '', description, flags=re.DOTALL).strip()\n        \n        if not description:\n            description = \"Unable to generate scene description.\"\n        \n        return description\n    \n    except Exception as e:\n        return f\"Error generating scene description: {str(e)}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:45:05.465774Z","iopub.execute_input":"2025-07-28T12:45:05.466025Z","iopub.status.idle":"2025-07-28T12:45:05.482620Z","shell.execute_reply.started":"2025-07-28T12:45:05.466002Z","shell.execute_reply":"2025-07-28T12:45:05.481942Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Process Traffic Image","metadata":{}},{"cell_type":"code","source":"def process_traffic_image(image, yolo_confidence=0.5, ocr_confidence=0.1, vllm_temperature=0.7, vllm_top_p=0.9):\n    \"\"\"Main processing function with adjustable parameters\"\"\"\n    if image is None:\n        return \"Please upload an image\", \"\", \"\"\n    \n    try:\n        # Load models\n        yolo_model, llava_model, processor, ocr_model = load_models()\n        \n        # Process tasks with user-specified parameters\n        print(f\"Processing image with parameters:\")\n        print(f\"  YOLO confidence: {yolo_confidence}\")\n        print(f\"  OCR confidence: {ocr_confidence}\")\n        print(f\"  VLLM temperature: {vllm_temperature}\")\n        print(f\"  VLLM top_p: {vllm_top_p}\")\n        \n        # Detect license plates with adjustable confidence\n        print(\"Detecting license plates...\")\n        plate_detections = detect_license_plates(image, yolo_model, yolo_confidence)\n        \n        # Generate scene description with adjustable parameters\n        print(\"Generating scene description...\")\n        scene_description = describe_traffic_scene(image, llava_model, processor, vllm_temperature, vllm_top_p)\n        \n        # Process each detected plate with OCR using adjustable confidence\n        print(f\"Processing {len(plate_detections)} detected plates...\")\n        processed_plates = []\n        for i, detection in enumerate(plate_detections):\n            try:\n                bbox = detection['bbox']\n                plate_text, ocr_conf = extract_text_from_plate(image, bbox, ocr_model, ocr_confidence)\n                \n                print(f\"Final OCR result for plate {i+1}: text='{plate_text}', conf={ocr_conf:.3f}\")\n                \n                # Include plates with any readable text OR high detection confidence\n                if plate_text and len(plate_text) >= 2:\n                    processed_plates.append({\n                        'bbox': bbox,\n                        'detection_confidence': detection['confidence'],\n                        'plate_text': plate_text,\n                        'ocr_confidence': ocr_conf\n                    })\n                    print(f\"✅ Added readable plate {i+1}: '{plate_text}' (conf: {ocr_conf:.3f})\")\n                elif detection['confidence'] > 0.7:  # High detection confidence even if OCR failed\n                    processed_plates.append({\n                        'bbox': bbox,\n                        'detection_confidence': detection['confidence'],\n                        'plate_text': f\"[High confidence detection - OCR failed]\",\n                        'ocr_confidence': 0.0\n                    })\n                    print(f\"⚠️ Added high-confidence unreadable plate {i+1}\")\n                else:\n                    # Low confidence detection\n                    processed_plates.append({\n                        'bbox': bbox,\n                        'detection_confidence': detection['confidence'],\n                        'plate_text': f\"[Low confidence detection - conf: {detection['confidence']:.3f}]\",\n                        'ocr_confidence': 0.0\n                    })\n                    print(f\"⚠️ Added low-confidence plate {i+1}\")\n            except Exception as e:\n                print(f\"Error processing plate {i+1}: {str(e)}\")\n                import traceback\n                traceback.print_exc()\n                # Still add the detection as unreadable\n                processed_plates.append({\n                    'bbox': detection['bbox'],\n                    'detection_confidence': detection['confidence'],\n                    'plate_text': \"[Processing error]\",\n                    'ocr_confidence': 0.0\n                })\n                continue\n        \n        # Create final JSON output with parameter info\n        final_result = {\n            'scene_description': scene_description,\n            'total_plates_detected': len(processed_plates),\n            'license_plates': processed_plates,\n            'parameters_used': {\n                'yolo_confidence_threshold': yolo_confidence,\n                'ocr_confidence_threshold': ocr_confidence,\n                'vllm_temperature': vllm_temperature,\n                'vllm_top_p': vllm_top_p\n            },\n        }\n        \n        # Format outputs for Gradio\n        scene_text = f\"Scene Description (temp={vllm_temperature}, top_p={vllm_top_p}):\\n{scene_description}\"\n        plates_text = f\"License Plates Detected: {len(processed_plates)} (from {len(plate_detections)} YOLO detections)\\n\"\n        plates_text += f\"YOLO Confidence Threshold: {yolo_confidence:.2f}\\n\"\n        plates_text += f\"OCR Confidence Threshold: {ocr_confidence:.2f}\\n\\n\"\n        \n        if processed_plates:\n            for i, plate in enumerate(processed_plates, 1):\n                plates_text += f\"Plate {i}:\\n\"\n                plates_text += f\"  Text: '{plate['plate_text']}'\\n\"\n                plates_text += f\"  Detection Confidence: {plate['detection_confidence']:.3f}\\n\"\n                plates_text += f\"  OCR Confidence: {plate['ocr_confidence']:.3f}\\n\"\n                plates_text += f\"  Bounding Box: {plate['bbox']}\\n\\n\"\n        else:\n            plates_text += \"No license plates detected by YOLO model.\\n\"\n            \n        json_output = json.dumps(final_result, indent=2)\n        \n        return scene_text, plates_text, json_output\n        \n    except Exception as e:\n        return f\"Error processing image: {str(e)}\", \"\", \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:45:05.483367Z","iopub.execute_input":"2025-07-28T12:45:05.483568Z","iopub.status.idle":"2025-07-28T12:45:05.501821Z","shell.execute_reply.started":"2025-07-28T12:45:05.483554Z","shell.execute_reply":"2025-07-28T12:45:05.501094Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Gradio","metadata":{}},{"cell_type":"code","source":"def create_gradio_interface():\n    \"\"\"Create the Gradio interface with adjustable parameters\"\"\"\n    \n    # Define the interface with parameter controls\n    iface = gr.Interface(\n        fn=process_traffic_image,\n        inputs=[\n            gr.Image(type=\"pil\", label=\"Upload Traffic Scene Image\"),\n            gr.Slider(\n                minimum=0.1, \n                maximum=1.0, \n                value=0.5, \n                step=0.05,\n                label=\"YOLO Detection Confidence Threshold\",\n                info=\"Higher values = fewer but more confident detections\"\n            ),\n            gr.Slider(\n                minimum=0.01, \n                maximum=1.0, \n                value=0.1, \n                step=0.01,\n                label=\"OCR Confidence Threshold\",\n                info=\"Minimum confidence required to accept OCR text results\"\n            ),\n            gr.Slider(\n                minimum=0.0, \n                maximum=1.0, \n                value=0.7, \n                step=0.1,\n                label=\"VLLM Temperature\",\n                info=\"Controls randomness in scene description\"\n            ),\n            gr.Slider(\n                minimum=0.1, \n                maximum=1.0, \n                value=0.9, \n                step=0.05,\n                label=\"VLLM Top-p\",\n                info=\"Controls diversity of scene description vocabulary\"\n            )\n        ],\n        outputs=[\n            gr.Textbox(\n                label=\"Traffic Scene Description\", \n                lines=8,\n                max_lines=15\n            ),\n            gr.Textbox(\n                label=\"License Plate Detection Results\", \n                lines=10,\n                max_lines=20\n            ),\n            gr.Textbox(\n                label=\"Complete JSON Output\", \n                lines=15,\n                max_lines=25\n            )\n        ],\n        title=\"🚗 Traffic Scene Analyzer with Adjustable Parameters\",\n        description=\"\"\"\n        Upload a traffic scene image and adjust parameters to customize the analysis:\n        \n        - **🎯 YOLO Detection**: Controls how confident the model needs to be to detect license plates\n        - **📝 OCR Confidence**: Sets minimum confidence for accepting text recognition results  \n        - **🧠 VLLM Temperature**: Controls creativity vs consistency in scene descriptions\n        - **🎨 VLLM Top-p**: Controls vocabulary diversity in scene descriptions\n        \n        **Outputs:**\n        1. **Scene Description**: AI-generated description of the traffic scene\n        2. **License Plate Detection**: Automatic detection and text extraction from license plates\n        3. **JSON Output**: Structured data combining both results with parameter information\n        \"\"\",\n        theme=gr.themes.Soft(),\n        allow_flagging=\"never\"\n    )\n    \n    return iface","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:45:05.503457Z","iopub.execute_input":"2025-07-28T12:45:05.503848Z","iopub.status.idle":"2025-07-28T12:45:05.518884Z","shell.execute_reply.started":"2025-07-28T12:45:05.503830Z","shell.execute_reply":"2025-07-28T12:45:05.518217Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Launch App","metadata":{}},{"cell_type":"code","source":"# Main execution\nif __name__ == \"__main__\":\n    print(\"🚀 Starting Enhanced Traffic Scene Analyzer...\")    \n    # Create and launch the interface\n    interface = create_gradio_interface()\n    \n    # Launch with public sharing for Colab\n    interface.launch(\n        share=True,\n        debug=True,\n        server_name=\"0.0.0.0\",\n        server_port=7860\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T12:45:05.519877Z","iopub.execute_input":"2025-07-28T12:45:05.520127Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Enhanced Traffic Scene Analyzer...\n* Running on local URL:  http://0.0.0.0:7860\n* Running on public URL: https://310dc8cc49b3dc2747.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://310dc8cc49b3dc2747.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Loading models...\n✅ YOLO model loaded successfully\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/176 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34d5e7a5ed714f6d886620a000ba4620"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a92b0b93216541609372505bcaaefbd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff65cb3c83444eaf8f4cd3af2e8f0e46"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dfaed13f1b44389a955d2d6f6ab3850"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5dde1018aff44fdbf28de6276a4bd3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac3cc6980554934a092aaca940e55a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fd2b87573984603ace697e54bbb9f6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015e38f8126f45f39292e7f3ce0fd6ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"816bf1a300e64f1f83c3e7e056370eff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"078f060ec425490bb128583e5a53681c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee41f5f928fe4ea588112cf1df0c2599"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92cece118bbb4699b3da40695862eb1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb2b444db234bdd97f27383ee8113d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"105cd5d874b445dcbb4903a656d1e387"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ca46d8e6ab44cbbd655d7c4b0bffdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcd53065c8664ff19c614363382daabd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30dd072a8d214fba840f00dd7bee3bbe"}},"metadata":{}},{"name":"stdout","text":"✅ LLaVA-NeXT model loaded successfully\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n\u001b[32mUsing official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /root/.paddlex/official_models.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfadf545832842398fc13f1305768042"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75326e3e92604df3b45fa8a906d7b9b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0479adfefa24ba4982883188e3bd0da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13afd1f415ea44b89586e06ac1f44978"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b49fb492c2b4455aad9580c27063a1ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.pdiparams:   0%|          | 0.00/6.75M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46737d14dc164c68b87c0a0d3532630f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.yml:   0%|          | 0.00/766 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6df753afa664613b28ed432f56aa1f8"}},"metadata":{}},{"name":"stderr","text":"\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n\u001b[33mThe model(UVDoc) is not supported to run in MKLDNN mode! Using `paddle` instead!\u001b[0m\n\u001b[32mUsing official model (UVDoc), the model files will be automatically downloaded and saved in /root/.paddlex/official_models.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dac46951e58a4e03b04015a5093d80f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b3674c736a4412496b1d1c30d283c57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d73b659c795e4b13aeddc503ef01bcea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cea919ef678d40ba91b49ab0d6d531f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f9044d592884924bd93e13a27112f41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.yml:   0%|          | 0.00/330 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3225c15829644d49a4ae3fae5c897344"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.pdiparams:   0%|          | 0.00/32.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a36c821feed43549411dd1092bc7853"}},"metadata":{}},{"name":"stderr","text":"\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n\u001b[32mUsing official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /root/.paddlex/official_models.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e515fad1a2c44b74b49ba909ab908418"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b47f73451c7d43a0b85dc0409cb59e0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.yml:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ccbd326f6894f8aabb9cc2382518fed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"248a8afbb20645d7a72319b89a19230d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ce248d265604e1281271588ee46347d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5beb581f95b41e3a9ab9c53744f939c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.pdiparams:   0%|          | 0.00/6.74M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65bae672a0ac4f4cbbdbc30200eee01d"}},"metadata":{}},{"name":"stderr","text":"\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n\u001b[32mUsing official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /root/.paddlex/official_models.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"597c4ec8eaa6411c96c9b4475da63c64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"161f7fe9581b405296011ed07ea8d718"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abb88c839a4e40c5a176b5eb5d0d3b54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.yml:   0%|          | 0.00/903 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"201526fc10e24d22a8a377f4eba1fe5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8da4f226f26497496feadb7fbbf1e56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.pdiparams:   0%|          | 0.00/87.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1389ea19923413e829affa110d684d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e910bba237b342cea90fe7426b265491"}},"metadata":{}},{"name":"stderr","text":"\u001b[32mCreating model: ('PP-OCRv5_server_rec', None)\u001b[0m\n\u001b[32mUsing official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /root/.paddlex/official_models.\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"612493e6203f48228cef636081950c81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb2d73dda40b41ad9c103a48c5e8f4fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.pdiparams:   0%|          | 0.00/84.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee4db8d0e1924b95af958415faba9e2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a25fb4d7557f4ec88025f5ab6058d221"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.yml: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc9b424a02754f409062fa4e4d261d99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"inference.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86b7792efb2844209d081bed76b17980"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abae62a641fe421ea3b7588e99877efa"}},"metadata":{}},{"name":"stdout","text":"✅ PaddleOCR model loaded successfully\nProcessing image with parameters:\n  YOLO confidence: 0.5\n  OCR confidence: 0.1\n  VLLM temperature: 0.7\n  VLLM top_p: 0.6\nDetecting license plates...\nRunning YOLO detection with confidence threshold: 0.5\n\n0: 448x640 1 License_Plate, 80.9ms\nSpeed: 55.3ms preprocess, 80.9ms inference, 247.3ms postprocess per image at shape (1, 3, 448, 640)\nFound 1 potential detections\nDetection 1: bbox=(292.8,165.7,378.3,213.3), size=(85.5x47.6), conf=0.804\n✅ Added detection 1\nFinal detections: 1\nGenerating scene description...\nGenerating scene description with temperature=0.7, top_p=0.6\nProcessing 1 detected plates...\nOriginal image size: 612x408\nRaw bbox: (292,165,378,213)\nClamped bbox: (292,165,378,213)\nFinal crop region: (282,155,388,223)\nCropped plate image size: (68, 106, 3)\nTrying OCR on original image with confidence threshold: 0.1\nHandling list format...\n  ✅ New best result: 'HLFACAR' (conf: 0.990)\nFinal result: 'HLFACAR' (conf: 0.990)\nFinal OCR result for plate 1: text='HLFACAR', conf=0.990\n✅ Added readable plate 1: 'HLFACAR' (conf: 0.990)\nProcessing image with parameters:\n  YOLO confidence: 0.5\n  OCR confidence: 0.1\n  VLLM temperature: 0.3\n  VLLM top_p: 0.3\nDetecting license plates...\nRunning YOLO detection with confidence threshold: 0.5\n\n0: 640x448 1 License_Plate, 49.0ms\nSpeed: 3.3ms preprocess, 49.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\nFound 1 potential detections\nDetection 1: bbox=(333.6,402.0,456.3,495.7), size=(122.7x93.7), conf=0.793\n✅ Added detection 1\nFinal detections: 1\nGenerating scene description...\nGenerating scene description with temperature=0.3, top_p=0.3\nProcessing 1 detected plates...\nOriginal image size: 605x881\nRaw bbox: (333,401,456,495)\nClamped bbox: (333,401,456,495)\nFinal crop region: (323,391,466,505)\nCropped plate image size: (114, 143, 3)\nTrying OCR on original image with confidence threshold: 0.1\nHandling list format...\n  ✅ New best result: 'OHIFIT' (conf: 0.991)\nFinal result: 'OHIFIT' (conf: 0.991)\nFinal OCR result for plate 1: text='OHIFIT', conf=0.991\n✅ Added readable plate 1: 'OHIFIT' (conf: 0.991)\nProcessing image with parameters:\n  YOLO confidence: 0.5\n  OCR confidence: 0.1\n  VLLM temperature: 0.3\n  VLLM top_p: 0.3\nDetecting license plates...\nRunning YOLO detection with confidence threshold: 0.5\n\n0: 384x640 2 License_Plates, 52.2ms\nSpeed: 3.0ms preprocess, 52.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\nFound 2 potential detections\nDetection 1: bbox=(592.2,1310.1,896.6,1490.0), size=(304.4x179.9), conf=0.793\n✅ Added detection 1\nDetection 2: bbox=(2857.2,1283.8,3126.8,1448.7), size=(269.7x164.8), conf=0.782\n✅ Added detection 2\nFinal detections: 2\nGenerating scene description...\nGenerating scene description with temperature=0.3, top_p=0.3\nProcessing 2 detected plates...\nOriginal image size: 1024x576\nRaw bbox: (592,1310,896,1489)\nClamped bbox: (592,575,896,576)\nFinal crop region: (582,565,906,576)\nCropped plate image size: (11, 324, 3)\nTrying OCR on original image with confidence threshold: 0.1\nHandling list format...\nTrying OCR on preprocessed image...\nProcessed OCR result type: <class 'list'>\nFinal result: '' (conf: 0.000)\nFinal OCR result for plate 1: text='', conf=0.000\n⚠️ Added high-confidence unreadable plate 1\nOriginal image size: 1024x576\nRaw bbox: (2857,1283,3126,1448)\nClamped bbox: (1023,575,1024,576)\nFinal crop region: (1013,565,1024,576)\nCropped plate image size: (11, 11, 3)\nTrying OCR on original image with confidence threshold: 0.1\nHandling list format...\nTrying OCR on preprocessed image...\nProcessed OCR result type: <class 'list'>\nFinal result: '' (conf: 0.000)\nFinal OCR result for plate 2: text='', conf=0.000\n⚠️ Added high-confidence unreadable plate 2\nProcessing image with parameters:\n  YOLO confidence: 0.5\n  OCR confidence: 0.5\n  VLLM temperature: 0.5\n  VLLM top_p: 0.5\nDetecting license plates...\nRunning YOLO detection with confidence threshold: 0.5\n\n0: 448x640 1 License_Plate, 13.0ms\nSpeed: 2.1ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\nFound 1 potential detections\nDetection 1: bbox=(98.6,83.0,249.9,140.3), size=(151.3x57.3), conf=0.787\n✅ Added detection 1\nFinal detections: 1\nGenerating scene description...\nGenerating scene description with temperature=0.5, top_p=0.5\nProcessing 1 detected plates...\nOriginal image size: 353x234\nRaw bbox: (98,82,249,140)\nClamped bbox: (98,82,249,140)\nFinal crop region: (88,72,259,150)\nCropped plate image size: (78, 171, 3)\nTrying OCR on original image with confidence threshold: 0.5\nHandling list format...\n  ✅ New best result: '3NZA760' (conf: 0.996)\nFinal result: '3NZA760' (conf: 0.996)\nFinal OCR result for plate 1: text='3NZA760', conf=0.996\n✅ Added readable plate 1: '3NZA760' (conf: 0.996)\nProcessing image with parameters:\n  YOLO confidence: 0.5\n  OCR confidence: 0.5\n  VLLM temperature: 1\n  VLLM top_p: 0.5\nDetecting license plates...\nRunning YOLO detection with confidence threshold: 0.5\n\n0: 448x640 1 License_Plate, 15.0ms\nSpeed: 3.2ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\nFound 1 potential detections\nDetection 1: bbox=(98.6,83.0,249.9,140.3), size=(151.3x57.3), conf=0.787\n✅ Added detection 1\nFinal detections: 1\nGenerating scene description...\nGenerating scene description with temperature=1, top_p=0.5\nProcessing 1 detected plates...\nOriginal image size: 353x234\nRaw bbox: (98,82,249,140)\nClamped bbox: (98,82,249,140)\nFinal crop region: (88,72,259,150)\nCropped plate image size: (78, 171, 3)\nTrying OCR on original image with confidence threshold: 0.5\nHandling list format...\n  ✅ New best result: '3NZA760' (conf: 0.996)\nFinal result: '3NZA760' (conf: 0.996)\nFinal OCR result for plate 1: text='3NZA760', conf=0.996\n✅ Added readable plate 1: '3NZA760' (conf: 0.996)\nProcessing image with parameters:\n  YOLO confidence: 0.5\n  OCR confidence: 0.5\n  VLLM temperature: 1\n  VLLM top_p: 1\nDetecting license plates...\nRunning YOLO detection with confidence threshold: 0.5\n\n0: 448x640 1 License_Plate, 12.4ms\nSpeed: 2.1ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\nFound 1 potential detections\nDetection 1: bbox=(98.6,83.0,249.9,140.3), size=(151.3x57.3), conf=0.787\n✅ Added detection 1\nFinal detections: 1\nGenerating scene description...\nGenerating scene description with temperature=1, top_p=1\nProcessing 1 detected plates...\nOriginal image size: 353x234\nRaw bbox: (98,82,249,140)\nClamped bbox: (98,82,249,140)\nFinal crop region: (88,72,259,150)\nCropped plate image size: (78, 171, 3)\nTrying OCR on original image with confidence threshold: 0.5\nHandling list format...\n  ✅ New best result: '3NZA760' (conf: 0.996)\nFinal result: '3NZA760' (conf: 0.996)\nFinal OCR result for plate 1: text='3NZA760', conf=0.996\n✅ Added readable plate 1: '3NZA760' (conf: 0.996)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}