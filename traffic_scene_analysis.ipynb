{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 492603,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 391538,
          "modelId": 410251
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install required packages"
      ],
      "metadata": {
        "id": "7lTqWX68f1zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio ultralytics paddlepaddle paddleocr transformers torch torchvision accelerate bitsandbytes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-28T13:41:42.188214Z",
          "iopub.execute_input": "2025-07-28T13:41:42.188397Z",
          "iopub.status.idle": "2025-07-28T13:43:37.684575Z",
          "shell.execute_reply.started": "2025-07-28T13:41:42.188379Z",
          "shell.execute_reply": "2025-07-28T13:43:37.683878Z"
        },
        "id": "5aSDrPpdf1zn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "ZYdCNvVMf1zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import cv2\n",
        "import json\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "from ultralytics import YOLO\n",
        "from paddleocr import PaddleOCR\n",
        "from transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig\n",
        "import warnings\n",
        "import re\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-28T13:43:37.686328Z",
          "iopub.execute_input": "2025-07-28T13:43:37.686595Z",
          "iopub.status.idle": "2025-07-28T13:44:23.755975Z",
          "shell.execute_reply.started": "2025-07-28T13:43:37.686571Z",
          "shell.execute_reply": "2025-07-28T13:44:23.755319Z"
        },
        "id": "gkUTvOxLf1zo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Models"
      ],
      "metadata": {
        "id": "SIGAEr34f1zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variables to store loaded models\n",
        "MODELS_LOADED = False\n",
        "yolo_model = None\n",
        "llava_model = None\n",
        "processor = None\n",
        "ocr_model = None"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-28T13:44:23.756914Z",
          "iopub.execute_input": "2025-07-28T13:44:23.757716Z",
          "iopub.status.idle": "2025-07-28T13:44:23.761116Z",
          "shell.execute_reply.started": "2025-07-28T13:44:23.757697Z",
          "shell.execute_reply": "2025-07-28T13:44:23.760378Z"
        },
        "id": "dVXzxr-lf1zp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models():\n",
        "    \"\"\"Load all required models once\"\"\"\n",
        "    global MODELS_LOADED, yolo_model, llava_model, processor, ocr_model\n",
        "\n",
        "    if MODELS_LOADED:\n",
        "        return yolo_model, llava_model, processor, ocr_model\n",
        "\n",
        "    print(\"Loading models...\")\n",
        "\n",
        "    try:\n",
        "        # Load YOLO model for license plate detection\n",
        "        yolo_model_path = \"/kaggle/input/license_plate_detect_yolo11/pytorch/default/1/best.pt\"\n",
        "        yolo_model = YOLO(yolo_model_path)\n",
        "        print(\"‚úÖ YOLO model loaded successfully\")\n",
        "\n",
        "        # Load LLaVA-NeXT model with quantization for faster inference\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "\n",
        "        processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
        "        llava_model = AutoModelForImageTextToText.from_pretrained(\n",
        "            \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
        "            quantization_config=quant_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        print(\"‚úÖ LLaVA-NeXT model loaded successfully\")\n",
        "\n",
        "        # Initialize PaddleOCR\n",
        "        ocr_model = PaddleOCR(\n",
        "            use_angle_cls=True,\n",
        "            lang='en'\n",
        "        )\n",
        "        print(\"‚úÖ PaddleOCR model loaded successfully\")\n",
        "\n",
        "        MODELS_LOADED = True\n",
        "        return yolo_model, llava_model, processor, ocr_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading models: {str(e)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-28T13:44:23.761807Z",
          "iopub.execute_input": "2025-07-28T13:44:23.762054Z",
          "iopub.status.idle": "2025-07-28T13:44:23.838079Z",
          "shell.execute_reply.started": "2025-07-28T13:44:23.762036Z",
          "shell.execute_reply": "2025-07-28T13:44:23.837560Z"
        },
        "id": "9WBbCdTWf1zp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_model, llava_model, processor, ocr_model = load_models()\n",
        "MODELS_LOADED = True"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-28T13:44:23.839969Z",
          "iopub.execute_input": "2025-07-28T13:44:23.840339Z"
        },
        "id": "OENv1DPNf1zp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detect License Plates"
      ],
      "metadata": {
        "id": "9yWeZg-Af1zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_license_plates(image, yolo_model, confidence_threshold=0.5):\n",
        "    \"\"\"Detect license plates using YOLO model with adjustable confidence threshold\"\"\"\n",
        "    try:\n",
        "        # Convert PIL image to numpy array\n",
        "        if isinstance(image, Image.Image):\n",
        "            image_np = np.array(image)\n",
        "        else:\n",
        "            image_np = image\n",
        "\n",
        "        print(f\"Running YOLO detection with confidence threshold: {confidence_threshold}\")\n",
        "\n",
        "        # Run YOLO inference with user-specified confidence threshold\n",
        "        results = yolo_model(\n",
        "            image_np,\n",
        "            conf=confidence_threshold,\n",
        "            imgsz=640,\n",
        "            half=False,  # Disable FP16 for better compatibility\n",
        "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        detections = []\n",
        "        for result in results:\n",
        "            boxes = result.boxes\n",
        "            if boxes is not None:\n",
        "                print(f\"Found {len(boxes)} potential detections\")\n",
        "                for i, box in enumerate(boxes):\n",
        "                    # Get bounding box coordinates\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                    confidence = box.conf[0].cpu().numpy()\n",
        "\n",
        "                    # More lenient size filter\n",
        "                    width = x2 - x1\n",
        "                    height = y2 - y1\n",
        "                    print(f\"Detection {i+1}: bbox=({x1:.1f},{y1:.1f},{x2:.1f},{y2:.1f}), size=({width:.1f}x{height:.1f}), conf={confidence:.3f}\")\n",
        "\n",
        "                    if width > 15 and height > 8:  # More lenient minimum size filter\n",
        "                        detections.append({\n",
        "                            'bbox': [int(x1), int(y1), int(x2), int(y2)],\n",
        "                            'confidence': float(confidence)\n",
        "                        })\n",
        "                        print(f\"‚úÖ Added detection {i+1}\")\n",
        "                    else:\n",
        "                        print(f\"‚ùå Filtered out detection {i+1} (too small)\")\n",
        "            else:\n",
        "                print(\"No boxes detected by YOLO\")\n",
        "\n",
        "        print(f\"Final detections: {len(detections)}\")\n",
        "        return detections\n",
        "    except Exception as e:\n",
        "        print(f\"Error in license plate detection: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "trusted": true,
        "id": "F6k-JbBtf1zq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess License Plates"
      ],
      "metadata": {
        "id": "EugulUeDf1zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_plate_image(plate_image):\n",
        "    \"\"\"Preprocess license plate image for better OCR\"\"\"\n",
        "    try:\n",
        "        # Ensure we have a valid image\n",
        "        if plate_image is None or plate_image.size == 0:\n",
        "            return None\n",
        "\n",
        "        # Convert to grayscale if needed\n",
        "        if len(plate_image.shape) == 3:\n",
        "            gray = cv2.cvtColor(plate_image, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = plate_image\n",
        "\n",
        "        # Resize if too small\n",
        "        h, w = gray.shape\n",
        "        if h < 32 or w < 64:\n",
        "            scale_factor = max(32/h, 64/w, 2.0)\n",
        "            new_h, new_w = int(h * scale_factor), int(w * scale_factor)\n",
        "            gray = cv2.resize(gray, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        # Apply CLAHE for better contrast\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        enhanced = clahe.apply(gray)\n",
        "\n",
        "        # Apply Gaussian blur to reduce noise\n",
        "        blurred = cv2.GaussianBlur(enhanced, (3, 3), 0)\n",
        "\n",
        "        # Apply threshold\n",
        "        _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "        # Convert back to 3-channel for PaddleOCR compatibility\n",
        "        thresh_3ch = cv2.cvtColor(thresh, cv2.COLOR_GRAY2RGB)\n",
        "        return thresh_3ch\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing plate image: {str(e)}\")\n",
        "        return plate_image"
      ],
      "metadata": {
        "trusted": true,
        "id": "oWLq3LSaf1zq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Plate Text"
      ],
      "metadata": {
        "id": "cdKZWxief1zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_plate(image, bbox, ocr_model, ocr_confidence_threshold=0.1):\n",
        "    \"\"\"Extract text from license plate using OCR with adjustable confidence threshold\"\"\"\n",
        "    try:\n",
        "        x1, y1, x2, y2 = bbox\n",
        "\n",
        "        # Convert PIL to numpy if needed\n",
        "        if isinstance(image, Image.Image):\n",
        "            image_np = np.array(image)\n",
        "        else:\n",
        "            image_np = image\n",
        "\n",
        "        # Get original image dimensions\n",
        "        h, w = image_np.shape[:2]\n",
        "        print(f\"Original image size: {w}x{h}\")\n",
        "        print(f\"Raw bbox: ({x1},{y1},{x2},{y2})\")\n",
        "\n",
        "        # Clamp coordinates to image boundaries\n",
        "        x1_clamped = max(0, min(x1, w-1))\n",
        "        y1_clamped = max(0, min(y1, h-1))\n",
        "        x2_clamped = max(x1_clamped+1, min(x2, w))\n",
        "        y2_clamped = max(y1_clamped+1, min(y2, h))\n",
        "\n",
        "        print(f\"Clamped bbox: ({x1_clamped},{y1_clamped},{x2_clamped},{y2_clamped})\")\n",
        "\n",
        "        # Check if bbox is still valid after clamping\n",
        "        if x2_clamped <= x1_clamped or y2_clamped <= y1_clamped:\n",
        "            print(\"Invalid bbox coordinates after clamping\")\n",
        "            return \"\", 0.0\n",
        "\n",
        "        # Crop the license plate region with some padding\n",
        "        padding = 10\n",
        "        x1_pad = max(0, x1_clamped - padding)\n",
        "        y1_pad = max(0, y1_clamped - padding)\n",
        "        x2_pad = min(w, x2_clamped + padding)\n",
        "        y2_pad = min(h, y2_clamped + padding)\n",
        "\n",
        "        print(f\"Final crop region: ({x1_pad},{y1_pad},{x2_pad},{y2_pad})\")\n",
        "\n",
        "        plate_image = image_np[y1_pad:y2_pad, x1_pad:x2_pad]\n",
        "\n",
        "        # Check if plate_image is valid\n",
        "        if plate_image.size == 0:\n",
        "            print(\"Empty plate image after cropping\")\n",
        "            return \"\", 0.0\n",
        "\n",
        "        print(f\"Cropped plate image size: {plate_image.shape}\")\n",
        "\n",
        "        # Ensure the cropped image has the right format\n",
        "        if len(plate_image.shape) != 3:\n",
        "            print(\"Converting grayscale to RGB\")\n",
        "            if len(plate_image.shape) == 2:\n",
        "                plate_image = cv2.cvtColor(plate_image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        # Try OCR on original cropped image first\n",
        "        best_text = \"\"\n",
        "        best_confidence = 0.0\n",
        "\n",
        "        try:\n",
        "            print(f\"Trying OCR on original image with confidence threshold: {ocr_confidence_threshold}\")\n",
        "            ocr_results_original = ocr_model.ocr(plate_image)\n",
        "\n",
        "            # Handle the new PaddleOCR dict format\n",
        "            if isinstance(ocr_results_original, dict):\n",
        "                rec_texts = ocr_results_original.get('rec_texts', [])\n",
        "                rec_scores = ocr_results_original.get('rec_scores', [])\n",
        "\n",
        "                print(f\"Found rec_texts: {rec_texts}\")\n",
        "                print(f\"Found rec_scores: {rec_scores}\")\n",
        "\n",
        "                for i, text in enumerate(rec_texts):\n",
        "                    if i < len(rec_scores):\n",
        "                        conf = rec_scores[i]\n",
        "                        if text and len(str(text).strip()) > 0 and conf > ocr_confidence_threshold:\n",
        "                            print(f\"  Original - Text: '{text}', Confidence: {conf:.3f}\")\n",
        "\n",
        "                            # Clean up the text\n",
        "                            cleaned_text = re.sub(r'[^A-Z0-9]', '', str(text).upper())\n",
        "\n",
        "                            if cleaned_text and len(cleaned_text) >= 2:\n",
        "                                if conf > best_confidence:\n",
        "                                    best_text = cleaned_text\n",
        "                                    best_confidence = conf\n",
        "                                    print(f\"  ‚úÖ New best result: '{best_text}' (conf: {best_confidence:.3f})\")\n",
        "\n",
        "            # Handle list format (legacy or different version)\n",
        "            elif isinstance(ocr_results_original, list) and len(ocr_results_original) > 0:\n",
        "                print(\"Handling list format...\")\n",
        "\n",
        "                # Check if it's a list of detection results\n",
        "                if isinstance(ocr_results_original[0], list):\n",
        "                    for line in ocr_results_original[0]:\n",
        "                        if line and len(line) >= 2 and line[1]:\n",
        "                            text = str(line[1][0]).strip()\n",
        "                            conf = float(line[1][1]) if len(line[1]) >= 2 else 0.0\n",
        "\n",
        "                            print(f\"  Original (list) - Text: '{text}', Confidence: {conf:.3f}\")\n",
        "\n",
        "                            if conf > ocr_confidence_threshold and len(text) > 0:\n",
        "                                cleaned_text = re.sub(r'[^A-Z0-9]', '', text.upper())\n",
        "\n",
        "                                if cleaned_text and len(cleaned_text) >= 2:\n",
        "                                    if conf > best_confidence:\n",
        "                                        best_text = cleaned_text\n",
        "                                        best_confidence = conf\n",
        "                                        print(f\"  ‚úÖ New best result: '{best_text}' (conf: {best_confidence:.3f})\")\n",
        "\n",
        "                # Direct list of results\n",
        "                else:\n",
        "                    for item in ocr_results_original:\n",
        "                        if isinstance(item, dict):\n",
        "                            rec_texts = item.get('rec_texts', [])\n",
        "                            rec_scores = item.get('rec_scores', [])\n",
        "\n",
        "                            for i, text in enumerate(rec_texts):\n",
        "                                if i < len(rec_scores):\n",
        "                                    conf = rec_scores[i]\n",
        "                                    if text and len(str(text).strip()) > 0 and conf > ocr_confidence_threshold:\n",
        "                                        cleaned_text = re.sub(r'[^A-Z0-9]', '', str(text).upper())\n",
        "                                        if cleaned_text and len(cleaned_text) >= 2:\n",
        "                                            if conf > best_confidence:\n",
        "                                                best_text = cleaned_text\n",
        "                                                best_confidence = conf\n",
        "                                                print(f\"  ‚úÖ New best result: '{best_text}' (conf: {best_confidence:.3f})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with original OCR: {str(e)}\")\n",
        "\n",
        "\n",
        "        # Try OCR on preprocessed image\n",
        "        if best_confidence < 0.8:\n",
        "            try:\n",
        "                print(\"Trying OCR on preprocessed image...\")\n",
        "                processed_plate = preprocess_plate_image(plate_image)\n",
        "\n",
        "                if processed_plate is not None:\n",
        "                    ocr_results_processed = ocr_model.ocr(processed_plate)\n",
        "\n",
        "                    print(f\"Processed OCR result type: {type(ocr_results_processed)}\")\n",
        "\n",
        "                    # Handle the new PaddleOCR dict format\n",
        "                    if isinstance(ocr_results_processed, dict):\n",
        "                        rec_texts = ocr_results_processed.get('rec_texts', [])\n",
        "                        rec_scores = ocr_results_processed.get('rec_scores', [])\n",
        "\n",
        "                        print(f\"Processed rec_texts: {rec_texts}\")\n",
        "                        print(f\"Processed rec_scores: {rec_scores}\")\n",
        "\n",
        "                        for i, text in enumerate(rec_texts):\n",
        "                            if i < len(rec_scores):\n",
        "                                conf = rec_scores[i]\n",
        "                                if text and len(str(text).strip()) > 0 and conf > ocr_confidence_threshold:\n",
        "                                    print(f\"  Processed - Text: '{text}', Confidence: {conf:.3f}\")\n",
        "\n",
        "                                    # Clean up the text\n",
        "                                    cleaned_text = re.sub(r'[^A-Z0-9]', '', str(text).upper())\n",
        "\n",
        "                                    if cleaned_text and len(cleaned_text) >= 2:\n",
        "                                        if conf > best_confidence:\n",
        "                                            best_text = cleaned_text\n",
        "                                            best_confidence = conf\n",
        "                                            print(f\"  ‚úÖ New best result: '{best_text}' (conf: {best_confidence:.3f})\")\n",
        "\n",
        "                    # Handle other formats...\n",
        "                    elif isinstance(ocr_results_processed, list) and len(ocr_results_processed) > 0:\n",
        "                        # Similar logic as above for processed results\n",
        "                        if isinstance(ocr_results_processed[0], list):\n",
        "                            for line in ocr_results_processed[0]:\n",
        "                                if line and len(line) >= 2 and line[1]:\n",
        "                                    text = str(line[1][0]).strip()\n",
        "                                    conf = float(line[1][1]) if len(line[1]) >= 2 else 0.0\n",
        "\n",
        "                                    if conf > ocr_confidence_threshold and len(text) > 0:\n",
        "                                        cleaned_text = re.sub(r'[^A-Z0-9]', '', text.upper())\n",
        "                                        if cleaned_text and len(cleaned_text) >= 2:\n",
        "                                            if conf > best_confidence:\n",
        "                                                best_text = cleaned_text\n",
        "                                                best_confidence = conf\n",
        "                                                print(f\"  ‚úÖ New best result: '{best_text}' (conf: {best_confidence:.3f})\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error with preprocessed OCR: {str(e)}\")\n",
        "\n",
        "        print(f\"Final result: '{best_text}' (conf: {best_confidence:.3f})\")\n",
        "        return best_text, best_confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in OCR text extraction: {str(e)}\")\n",
        "        return \"\", 0.0"
      ],
      "metadata": {
        "trusted": true,
        "id": "MP6WI1iNf1zq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traffic Scene Description"
      ],
      "metadata": {
        "id": "_YdpLc50f1zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def describe_traffic_scene(image, llava_model, processor, temperature=0.7, top_p=0.9):\n",
        "    \"\"\"Generate traffic scene description using LLaVA-NeXT with adjustable temperature and top_p\"\"\"\n",
        "    try:\n",
        "        # Resize image if too large to speed up inference\n",
        "        if isinstance(image, Image.Image):\n",
        "            width, height = image.size\n",
        "            if width > 1024 or height > 1024:\n",
        "                image.thumbnail((1024, 1024), Image.Resampling.LANCZOS)\n",
        "\n",
        "        # Simplified prompt for faster processing\n",
        "        prompt = \"\"\"\n",
        "        USER: <image>\n",
        "        Analyze this traffic scene in detail. Describe:\n",
        "        1. Types of vehicles present (cars, trucks, motorcycles, etc.)\n",
        "        2. Traffic signs, signals, and road markings visible\n",
        "        3. Road conditions and infrastructure\n",
        "        4. Weather and lighting conditions\n",
        "        5. Overall traffic flow and density\n",
        "        6. Any notable safety considerations or hazards\n",
        "\n",
        "        ASSISTANT:\"\"\"\n",
        "\n",
        "        inputs = processor(image, prompt, return_tensors=\"pt\")\n",
        "\n",
        "        # Move to appropriate device\n",
        "        device = next(llava_model.parameters()).device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        print(f\"Generating scene description with temperature={temperature}, top_p={top_p}\")\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            # Use adjustable temperature and top_p parameters\n",
        "            output = llava_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=150,\n",
        "                do_sample=True if temperature > 0.0 else False,\n",
        "                temperature=temperature if temperature > 0.0 else None,\n",
        "                top_p=top_p if temperature > 0.0 else None,\n",
        "                pad_token_id=processor.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        full_response = processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract only the assistant's response\n",
        "        description = \"\"\n",
        "        if \"ASSISTANT:\" in full_response:\n",
        "            parts = full_response.split(\"ASSISTANT:\")\n",
        "            if len(parts) > 1:\n",
        "                description = parts[-1].strip()\n",
        "\n",
        "        # Clean up any remaining template artifacts\n",
        "        if description.startswith(\"[INST]\") or description.startswith(\"USER:\"):\n",
        "            # Try alternative extraction\n",
        "            lines = full_response.split('\\n')\n",
        "            for i, line in enumerate(lines):\n",
        "                if \"ASSISTANT:\" in line and i + 1 < len(lines):\n",
        "                    description = '\\n'.join(lines[i+1:]).strip()\n",
        "                    break\n",
        "\n",
        "        # Remove any remaining template markers\n",
        "        description = re.sub(r'\\[/?INST\\]', '', description).strip()\n",
        "        description = re.sub(r'USER:.*?ASSISTANT:', '', description, flags=re.DOTALL).strip()\n",
        "\n",
        "        if not description:\n",
        "            description = \"Unable to generate scene description.\"\n",
        "\n",
        "        return description\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating scene description: {str(e)}\""
      ],
      "metadata": {
        "trusted": true,
        "id": "wcPkmx5Sf1zr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process Traffic Image"
      ],
      "metadata": {
        "id": "Y6EOkSAlf1zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_traffic_image(image, yolo_confidence=0.5, ocr_confidence=0.1, vllm_temperature=0.7, vllm_top_p=0.9):\n",
        "    \"\"\"Main processing function with adjustable parameters\"\"\"\n",
        "    if image is None:\n",
        "        return \"Please upload an image\", \"\", \"\"\n",
        "\n",
        "    try:\n",
        "        # Load models\n",
        "        yolo_model, llava_model, processor, ocr_model = load_models()\n",
        "\n",
        "        # Process tasks with user-specified parameters\n",
        "        print(f\"Processing image with parameters:\")\n",
        "        print(f\"  YOLO confidence: {yolo_confidence}\")\n",
        "        print(f\"  OCR confidence: {ocr_confidence}\")\n",
        "        print(f\"  VLLM temperature: {vllm_temperature}\")\n",
        "        print(f\"  VLLM top_p: {vllm_top_p}\")\n",
        "\n",
        "        # Detect license plates with adjustable confidence\n",
        "        print(\"Detecting license plates...\")\n",
        "        plate_detections = detect_license_plates(image, yolo_model, yolo_confidence)\n",
        "\n",
        "        # Generate scene description with adjustable parameters\n",
        "        print(\"Generating scene description...\")\n",
        "        scene_description = describe_traffic_scene(image, llava_model, processor, vllm_temperature, vllm_top_p)\n",
        "\n",
        "        # Process each detected plate with OCR using adjustable confidence\n",
        "        print(f\"Processing {len(plate_detections)} detected plates...\")\n",
        "        processed_plates = []\n",
        "        for i, detection in enumerate(plate_detections):\n",
        "            try:\n",
        "                bbox = detection['bbox']\n",
        "                plate_text, ocr_conf = extract_text_from_plate(image, bbox, ocr_model, ocr_confidence)\n",
        "\n",
        "                print(f\"Final OCR result for plate {i+1}: text='{plate_text}', conf={ocr_conf:.3f}\")\n",
        "\n",
        "                # Include plates with any readable text OR high detection confidence\n",
        "                if plate_text and len(plate_text) >= 2:\n",
        "                    processed_plates.append({\n",
        "                        'bbox': bbox,\n",
        "                        'detection_confidence': detection['confidence'],\n",
        "                        'plate_text': plate_text,\n",
        "                        'ocr_confidence': ocr_conf\n",
        "                    })\n",
        "                    print(f\"‚úÖ Added readable plate {i+1}: '{plate_text}' (conf: {ocr_conf:.3f})\")\n",
        "                elif detection['confidence'] > 0.7:  # High detection confidence even if OCR failed\n",
        "                    processed_plates.append({\n",
        "                        'bbox': bbox,\n",
        "                        'detection_confidence': detection['confidence'],\n",
        "                        'plate_text': f\"[High confidence detection - OCR failed]\",\n",
        "                        'ocr_confidence': 0.0\n",
        "                    })\n",
        "                    print(f\"‚ö†Ô∏è Added high-confidence unreadable plate {i+1}\")\n",
        "                else:\n",
        "                    # Low confidence detection\n",
        "                    processed_plates.append({\n",
        "                        'bbox': bbox,\n",
        "                        'detection_confidence': detection['confidence'],\n",
        "                        'plate_text': f\"[Low confidence detection - conf: {detection['confidence']:.3f}]\",\n",
        "                        'ocr_confidence': 0.0\n",
        "                    })\n",
        "                    print(f\"‚ö†Ô∏è Added low-confidence plate {i+1}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing plate {i+1}: {str(e)}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                # Still add the detection as unreadable\n",
        "                processed_plates.append({\n",
        "                    'bbox': detection['bbox'],\n",
        "                    'detection_confidence': detection['confidence'],\n",
        "                    'plate_text': \"[Processing error]\",\n",
        "                    'ocr_confidence': 0.0\n",
        "                })\n",
        "                continue\n",
        "\n",
        "        # Create final JSON output with parameter info\n",
        "        final_result = {\n",
        "            'scene_description': scene_description,\n",
        "            'total_plates_detected': len(processed_plates),\n",
        "            'license_plates': processed_plates,\n",
        "            'parameters_used': {\n",
        "                'yolo_confidence_threshold': yolo_confidence,\n",
        "                'ocr_confidence_threshold': ocr_confidence,\n",
        "                'vllm_temperature': vllm_temperature,\n",
        "                'vllm_top_p': vllm_top_p\n",
        "            },\n",
        "        }\n",
        "\n",
        "        # Format outputs for Gradio\n",
        "        scene_text = f\"Scene Description (temp={vllm_temperature}, top_p={vllm_top_p}):\\n{scene_description}\"\n",
        "        plates_text = f\"License Plates Detected: {len(processed_plates)} (from {len(plate_detections)} YOLO detections)\\n\"\n",
        "        plates_text += f\"YOLO Confidence Threshold: {yolo_confidence:.2f}\\n\"\n",
        "        plates_text += f\"OCR Confidence Threshold: {ocr_confidence:.2f}\\n\\n\"\n",
        "\n",
        "        if processed_plates:\n",
        "            for i, plate in enumerate(processed_plates, 1):\n",
        "                plates_text += f\"Plate {i}:\\n\"\n",
        "                plates_text += f\"  Text: '{plate['plate_text']}'\\n\"\n",
        "                plates_text += f\"  Detection Confidence: {plate['detection_confidence']:.3f}\\n\"\n",
        "                plates_text += f\"  OCR Confidence: {plate['ocr_confidence']:.3f}\\n\"\n",
        "                plates_text += f\"  Bounding Box: {plate['bbox']}\\n\\n\"\n",
        "        else:\n",
        "            plates_text += \"No license plates detected by YOLO model.\\n\"\n",
        "\n",
        "        json_output = json.dumps(final_result, indent=2)\n",
        "\n",
        "        return scene_text, plates_text, json_output\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing image: {str(e)}\", \"\", \"\""
      ],
      "metadata": {
        "trusted": true,
        "id": "c_8-3dzOf1zr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio"
      ],
      "metadata": {
        "id": "d0grTdyJf1zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"Create the Gradio interface with adjustable parameters\"\"\"\n",
        "\n",
        "    # Define the interface with parameter controls\n",
        "    iface = gr.Interface(\n",
        "        fn=process_traffic_image,\n",
        "        inputs=[\n",
        "            gr.Image(type=\"pil\", label=\"Upload Traffic Scene Image\"),\n",
        "            gr.Slider(\n",
        "                minimum=0.1,\n",
        "                maximum=1.0,\n",
        "                value=0.5,\n",
        "                step=0.05,\n",
        "                label=\"YOLO Detection Confidence Threshold\",\n",
        "                info=\"Higher values = fewer but more confident detections\"\n",
        "            ),\n",
        "            gr.Slider(\n",
        "                minimum=0.01,\n",
        "                maximum=1.0,\n",
        "                value=0.1,\n",
        "                step=0.01,\n",
        "                label=\"OCR Confidence Threshold\",\n",
        "                info=\"Minimum confidence required to accept OCR text results\"\n",
        "            ),\n",
        "            gr.Slider(\n",
        "                minimum=0.0,\n",
        "                maximum=1.0,\n",
        "                value=0.7,\n",
        "                step=0.1,\n",
        "                label=\"VLLM Temperature\",\n",
        "                info=\"Controls randomness in scene description\"\n",
        "            ),\n",
        "            gr.Slider(\n",
        "                minimum=0.1,\n",
        "                maximum=1.0,\n",
        "                value=0.9,\n",
        "                step=0.05,\n",
        "                label=\"VLLM Top-p\",\n",
        "                info=\"Controls diversity of scene description vocabulary\"\n",
        "            )\n",
        "        ],\n",
        "        outputs=[\n",
        "            gr.Textbox(\n",
        "                label=\"Traffic Scene Description\",\n",
        "                lines=8,\n",
        "                max_lines=15\n",
        "            ),\n",
        "            gr.Textbox(\n",
        "                label=\"License Plate Detection Results\",\n",
        "                lines=10,\n",
        "                max_lines=20\n",
        "            ),\n",
        "            gr.Textbox(\n",
        "                label=\"Complete JSON Output\",\n",
        "                lines=15,\n",
        "                max_lines=25\n",
        "            )\n",
        "        ],\n",
        "        title=\"üöó Traffic Scene Analyzer with Adjustable Parameters\",\n",
        "        description=\"\"\"\n",
        "        Upload a traffic scene image and adjust parameters to customize the analysis:\n",
        "\n",
        "        - **üéØ YOLO Detection**: Controls how confident the model needs to be to detect license plates\n",
        "        - **üìù OCR Confidence**: Sets minimum confidence for accepting text recognition results\n",
        "        - **üß† VLLM Temperature**: Controls creativity vs consistency in scene descriptions\n",
        "        - **üé® VLLM Top-p**: Controls vocabulary diversity in scene descriptions\n",
        "\n",
        "        **Outputs:**\n",
        "        1. **Scene Description**: AI-generated description of the traffic scene\n",
        "        2. **License Plate Detection**: Automatic detection and text extraction from license plates\n",
        "        3. **JSON Output**: Structured data combining both results with parameter information\n",
        "        \"\"\",\n",
        "        theme=gr.themes.Soft(),\n",
        "        allow_flagging=\"never\"\n",
        "    )\n",
        "\n",
        "    return iface"
      ],
      "metadata": {
        "trusted": true,
        "id": "r1g3Jc3zf1zs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launch App"
      ],
      "metadata": {
        "id": "ZyystDtkf1zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Starting Enhanced Traffic Scene Analyzer...\")\n",
        "    # Create and launch the interface\n",
        "    interface = create_gradio_interface()\n",
        "\n",
        "    # Launch with public sharing for Colab\n",
        "    interface.launch(\n",
        "        share=True,\n",
        "        debug=True,\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "id": "MoaWdAGWf1zs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "CuKP4v95f1zs"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}